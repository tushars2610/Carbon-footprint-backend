import requests
from typing import List
import logging
import json
from tenacity import retry, stop_after_attempt, wait_exponential

def create_prompt(query: str, context: List[str], system_prompt: str) -> str:
    """
    Create a prompt for the DeepSeek LLM API.
    
    Args:
        query: The user query.
        context: List of retrieved document contents.
        system_prompt: The system prompt for the domain.
    
    Returns:
        Formatted prompt string.
    """
    context_str = "\n".join([f"Context {i+1}: {content}" for i, content in enumerate(context)])
    prompt = f"{system_prompt}\n\n{context_str}\n\nUser Query: {query}\nAnswer:"
    return prompt

def create_comprehensive_prompt(query: str, context: List[str], system_prompt: str, domain: str, has_knowledge_base: bool = True) -> str:
    """
    Create a comprehensive prompt that includes system prompt, knowledge base, and user query.
    This is the main function used by the new simplified query system.
    
    Args:
        query: The user query
        context: List of relevant document contents from knowledge base
        system_prompt: The domain-specific system prompt
        domain: The domain name
        has_knowledge_base: Whether relevant documents were found
    
    Returns:
        Formatted prompt string for the LLM
    """
    
    # Build the knowledge base section
    knowledge_base_section = ""
    if has_knowledge_base and context:
        knowledge_base_section = f"""
KNOWLEDGE BASE CONTEXT from {domain} domain:
"""
        for i, content in enumerate(context, 1):
            knowledge_base_section += f"\nDocument {i}:\n{content}\n"
    else:
        knowledge_base_section = f"\nNo specific documents found in the {domain} knowledge base for this query."
    
    # Create the comprehensive prompt
    comprehensive_prompt = f"""{system_prompt}

{knowledge_base_section}

USER QUERY: {query}

INSTRUCTIONS:
- Use the knowledge base context above to answer the user's query when relevant information is available
- If the knowledge base contains relevant information, provide a detailed answer based on that content
- If the knowledge base doesn't contain relevant information, you can engage in general conversation while being helpful
- Always be conversational and friendly
- When using information from the knowledge base, make sure your answer is accurate and based on the provided context
- If you're unsure about something not covered in the knowledge base, be honest about the limitations
- Maintain context of the conversation and respond naturally

RESPONSE:"""
    
    return comprehensive_prompt

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def generate_response(prompt: str) -> str:
    """
    Generate a response using the DeepSeek LLM API.
    
    Args:
        prompt: The input prompt.
    
    Returns:
        The LLM-generated response.
    """
    try:
        url = "https://apillm.mobiloittegroup.com/api/generate"
        headers = {"Content-Type": "application/json"}
        data = {
            "model": "mistral",
            "prompt": prompt,
            "stream": False
        }
        
        logging.info(f"Sending request to LLM API with prompt length: {len(prompt)}")
        response = requests.post(url, headers=headers, json=data, timeout=60)  # Increased timeout for complex queries
        response.raise_for_status()
        
        result = response.json()
        llm_response = result.get("response", "No response generated by the LLM.")
        
        logging.info(f"Received response from LLM API with length: {len(llm_response)}")
        return llm_response
        
    except requests.exceptions.Timeout:
        logging.error("LLM API request timed out")
        raise Exception("The request to the language model timed out. Please try again.")
    except requests.exceptions.ConnectionError:
        logging.error("Failed to connect to LLM API")
        raise Exception("Failed to connect to the language model service.")
    except requests.exceptions.HTTPError as e:
        logging.error(f"HTTP error from LLM API: {e}")
        raise Exception(f"Language model service returned an error: {e}")
    except Exception as e:
        logging.error(f"Error in LLM API call: {e}")
        raise

def handle_llm_errors(error: Exception) -> str:
    """
    Handle errors from the LLM API.
    
    Args:
        error: The exception raised.
    
    Returns:
        User-friendly error message.
    """
    logging.error(f"LLM error: {str(error)}")
    
    error_message = str(error).lower()
    
    if "timeout" in error_message:
        return "I'm sorry, but the response took too long to generate. Please try asking your question again, perhaps with fewer details."
    elif "connection" in error_message:
        return "I'm having trouble connecting to the language model service. Please try again in a moment."
    elif "service" in error_message or "api" in error_message:
        return "The language model service is currently unavailable. Please try again later."
    else:
        return "I encountered an issue while generating a response. Please try rephrasing your question or try again later."